{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictHealthcareFraud_s2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPfsFlOppLepsx6r5gq7Zdm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4p14xLMlUQV",
        "colab_type": "text"
      },
      "source": [
        "# Healthcare Provider Fraud Detection Machine Learning Model\n",
        "## June 8, 2020 \n",
        "## About RIHAD VARIAWA\n",
        "> As a Data Scientist and former head of global fintech research at Malastare.ai, I find fulfillment tacking challenges to solve complex problems using data\n",
        "\n",
        "![](https://media.giphy.com/media/3o7aD18vUgGvzh6eze/giphy.gif)\n",
        "\n",
        "# Overview\n",
        "It’s hard to imagine an industry better suited for applying state of the art *data analytics* than the **healthcare industry**. In 2017 US healthcare spending reached USD3.5 trillion, USD10,739 per person, or 17.9% of the nation's Gross Domestic Product. The complexity of the healthcare industry’s service structure and reimbursement processes relies on massive amounts of data to be shared through a network of organizations and systems. ***It’s not difficult to imagine that fraudulent activities would occur***. Estimates of total healthcare fraud range from 3% to 10% or USD105 billion per year at the most conservative level\n",
        "\n",
        "Modern claims environments are staffed to efficiently process the huge volumes of activity in a timely manner leaving few resources for fraud detection. And because of the complexity of the data, fraud detection is often managed by very experienced investigators whose limited bandwidth requires they concentrate on the largest cases. Still, many of the cases identified for attention rely on whistleblower actions or an offender becoming greedy, making a mistake, or coincidence\n",
        "\n",
        "> **Automated data modelling, analysis, and machine learning** holds the promise of tilting the balance in favor of more efficiently identifying fraud much earlier, reducing the amount of fraud and fraudulent payouts and allowing more resources to focus on the investigation, prosecution, and recovery of the fraud that has occurred\n",
        "\n",
        "This usecase examines scenarios that apply data analysis, visualization, and machine learning concepts to healthcare fraud\n",
        "\n",
        "# Problem Statement\n",
        "This is essentially a supervised classification problem as we assess important claims-related variables as indicators of potentially fraudulent behaviour. As claims data and fraudulent activity related to it is constantly being processed, it is best to view this as a fraud detection process that operates with constancy against the evolving claims activity. In the past, Data Analysis happened in a series of queries meant to investigate suspicious activity as human observation identified patterns in the data that indicate possible fraud. Ideally, state of the art Data Science and engineering can be combined to create a *data machine* that can continually evaluate the data for fraud indicators\n",
        "\n",
        "> ***The goal of this usecase is to illustrate how machine learning can be applied to address a healthcare fraud prevention challenge creating a solution that is more effective, efficient, scalable, and adaptable***\n",
        "\n",
        "# About the Dataset\n",
        "To provide realistic, complete, yet neutralized examples we’ll draw from a test dataset that includes inpatient, outpatient, and beneficiary data. We start with the raw or *test* data, then after applying some fraud detection logic to it, will produce *trained* data\n",
        "\n",
        "> ***The nature of each of the data objects are as explained below***:\n",
        "\n",
        "+ **Inpatient** data regarding claims filed for patients admitted to hospitals along with additional details like their admission and discharge dates\n",
        "+ **Outpatient** data regarding patients who visited the hospitals but were treated and released\n",
        "+ **Beneficiary** data covering KYC details like health conditions, the region they belong to etc\n",
        "+ **Provider** data which will also receive post-process data that would include a fraud or non-fraud classification based on their history of claims, more on that later\n",
        "\n",
        "# Workflow\n",
        "> 1 - Data collection and preparation: everything from choosing where to get the data, up to the point it is clean and ready for feature selection/engineering\n",
        "\n",
        "Data analysis requires effort to clean and merge related data points allowing it to be processed, in this case for fraud indicators\n",
        "\n",
        "It is meant to run continuously on live data and attention is paid on how data continually feeds the process. This must be done in such a way that new data elements can be easily adapted to support evolving healthcare needs without hampering the business processes that feed it\n",
        "\n",
        "> 2 - Feature selection and feature engineering: this includes all changes to the data from once it has been cleaned up to when it is ingested into the model\n",
        "\n",
        "Once the data clean-up is complete and datasets are merged, *data patterns* can be processed for fraud indicators. The data analysis environment has been established to support a continual data analysis process to happen in a live and evolving environment. The fraud detection logic and data visualization support must be developed and embedded into the process at this stage\n",
        "\n",
        "> ***Data science is the software of logical processes to understand and assess data to achieve a certain objective. In this case, it is important for data scientists to find the truth, not prove or disprove any particular theory. Don’t just find patterns, find meaningful patterns***\n",
        "\n",
        "Acquire a clear understanding of certain age groups, where most of the time false claims were being registered on common health issues\n",
        "\n",
        "![data-merging](https://drive.google.com/uc?export=download&id=18s7VHAVfN6qpfvtulA_SvdpC0Xo4C-dI)\n",
        "\n",
        "## Assumptions\n",
        "Part of the data clean-up activity requires us to proceed with certain assumptions depending on the scope of the project. In this case, given the limited scope of the task and known features of the data, some of the following assumptions and actions were undertaken\n",
        "\n",
        "+ Eliminated certain data points such as IDs and codes as they were deemed as not required for model building\n",
        "+ Only **Inpatient** data was considered as **Outpatient** cases might have additional conditions and parameters that were not apparent in this dataset\n",
        "+ Deep analysis of health conditions was not undertaken due to limited data\n",
        "\n",
        "![2nd](https://drive.google.com/uc?export=view&id=1vE-_OZCgPG9d-kY42VVmWpnI7nEIwvtI)\n",
        "\n",
        "### Process\n",
        "Upon completing the data pre-processing, Exploratory Data Analysis was undertaken on training dataset which classified the claims as **fraudulent Vs. non-fraudulent**\n",
        "\n",
        "+ Cases in the trained dataset were already classified as fraud or not fraud by looking after the combination of disorders patients have and the amount claimed for it\n",
        "\n",
        "![fig1](https://drive.google.com/uc?export=view&id=1WoEdU0-MZeWpu64s01RY9s1q6WV6QCVN)\n",
        "\n",
        "Inference - This plot shows the data distribution of non-fraud and fraud cases in the training set. The data is slightly imbalanced with non-fraudulent cases > fraudulent cases. The number of observations for non-fraud: 328,343 (61.9%), and the number of observations for fraud: 189,394 (38.1%)\n",
        "\n",
        "![fig2](https://drive.google.com/uc?export=view&id=1vM4b3QzbgvcEzP1iMg63Tgnm-t6pa1u2)\n",
        "\n",
        "Inference - This plot shows the amount claimed for non-fraudulent and fraudulent cases. Looking at the resulting plot it states that in comparison for a simpler disorder the provider is claiming more than (s)he should\n",
        "\n",
        "![fig3](https://drive.google.com/uc?export=view&id=1T0Ber7JWgWWIMxEfiA31i4Lkj2w2fPsF)\n",
        "\n",
        "Inference - This plot tells us about the age group for which most of the false claims are claimed. The graph indicates a higher degree of fraudulent cases in patients aged 60 and above\n",
        "\n",
        "![fig4](https://drive.google.com/uc?export=view&id=1Er3V6QC4nGNuFy1fHT1RYtOySzdBDFXh)\n",
        "\n",
        "Inference - This plot shows that on which disorder most of the false claims are made. Depression, heart and kidney related issues are the highest\n",
        "\n",
        "![fig5](https://drive.google.com/uc?export=view&id=1iZnHFzalU52NER-B4snj0Kh4UnBHUlxo)\n",
        "\n",
        "Inference - This plot tells us about the state code from where most of the false claims are registered. It can be seen from the plot that most of the fraud cases are registered from state code 5 followed by the state code 10, 33, 45, and 14\n",
        "\n",
        "# Model Building\n",
        "> 3 - Choosing the algorithm and training our first model: getting a better than baseline result upon which we can *(hopefully)* improve\n",
        "\n",
        "The basic requirement is to create a model from the cleaned and labeled data. The trained data was cleaned through data pre-processing techniques and labels were pre-existing. So based on the insights that came from the above analysis, a model was created on the trained dataset\n",
        "\n",
        "To observe the accuracy of the model, the trained dataset was **split** into a ratio of 70:30, named train and validation respectively. Our model was then created on the new unseen data and was used to predict the status of the providers on the validation data, in which the status of the providers were already been classified. This was just **to observe the results and compare them with the actual values that were already classified**\n",
        "\n",
        "Two models - **Logistic Regression and Random Forest Classifier** were created. With the performance of the Random Forest Classifier being better than Logistic Regression, it was applied to predict the status of the providers for the claims claimed on the **test data**\n",
        "\n",
        "The following table details the results obtained when executing the two models separately\n",
        "\n",
        "> 4 - Evaluating our model: this includes the selection of the metric as well as the actual evaluation; seemingly a smaller step than others, but important to our end result\n",
        "\n",
        "Model Metrics indicate the values that were being measured and Accuracy outlines the effectiveness of the individual model\n",
        "\n",
        "|  Label        | Precision     | Recall  | F1-score      |\n",
        "| ------------- |:-------------:| -------:|:-------------:|\n",
        "| 0 (non fraud) |     0.64      |  0.46   |    0.53       |\n",
        "| 1 (fraud)     |     0.37      |  0.55   |    0.44       |\n",
        "\n",
        "+ **Precision** refers to the percentage of relevant results\n",
        "+ **Recall** refers to the percentage of total relevant results correctly classified by the algorithm\n",
        "+ **F1-score** is the weighted average of the above two. Result tending to 1 being good, and tending to 0 being worse\n",
        "\n",
        "Accuracy Achieved - Random Forest Classifier predictions on validation dataset\n",
        "\n",
        "|  Model                    | Accuracy      |\n",
        "| ------------------------- |:-------------:|\n",
        "| Logistic Regression       |     50%       | \n",
        "| Random Forest Classifier  |     66%       |\n",
        "\n",
        "We can see that the best fit model is Random forest with Accuracy of 66%\n",
        "\n",
        "![fig6](https://drive.google.com/uc?export=view&id=1e19BCIPwSiXpOUWCMpwCJvonOJvDVY4C)\n",
        "\n",
        "Inference - This plot tells us about the priority given by the model to the variables to detect the target i.e status of the provider. The importance of the variables in ascending order is shown in the above plot, where the x-axis denotes the mean increase in the accuracy\n",
        "\n",
        "# Conclusion\n",
        "This analysis using a sample dataset was done to test the possibility of identifying fraudulent claims using past claim data. Having such a dataset would allow us to design, train and test machine learning models that enable us to extract intelligence embedded in large amounts of data that is overlooked otherwise. However, without an efficient way to integrate this model with your data, this only solves the problem partially. This is where *Rockwall Analytics* opens new possibilities \n",
        "\n",
        "Along with features such as continuous monitoring and automated alerts, we can automate an otherwise manual and tedious process such as data analysis and help you build intelligent frameworks from your most valuable resource, **your data**\n",
        "\n",
        "# About Rockwall Analytics \n",
        "*Rockwall Analytics* is a venture-funded startup \n",
        " \n",
        "Our mission is to enable organizations to make an easier transition into the field of Data Analytics and Artificial Intelligence. We aim to achieve this more efficiently than an organization might expect, were it to invest in the manpower and technology to build such complex platforms. For a universal need such as Data Science and Ai, a dedicated focus is absolutely critical and *Rockwall Analytics* empowers our clients with the knowledge that allows them to apply in their core business offerings\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1i7fzIUxz-oEs8V4uMdoZCQUl51NMrbVz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWC0y9c7rGjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "3dd5f438-f7b3-40a4-ae22-f35f065a3f03"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.22-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.22-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.22-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2apgd7-uVhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p colabData\n",
        "!google-drive-ocamlfuse colabData"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}